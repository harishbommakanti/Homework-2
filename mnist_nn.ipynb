{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "#this might take a while as it will download the dataset from internet\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "data_train = torchvision.datasets.MNIST('./', download=True, train=True, transform = transform)\n",
    "data_test = torchvision.datasets.MNIST('./', download=True, train=False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data loaders\n",
    "trainloader = torch.utils.data.DataLoader(data_train, batch_size=4, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(data_test, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining classes: for MNIST, numbers 0-9\n",
    "classes = tuple(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# input_size = 1 x 28 x 28\n",
    "input_size = data_train[0][0].shape.numel()\n",
    "# print(input_size)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # get data from input channels and upscale it to 784\n",
    "        # now, trim it from 784 down to 10, progressively\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get 3 channel image and transform it to 784 channel data\n",
    "        x = x.view(-1, input_size)\n",
    "        # use the ReLU activation function for the first 2 FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # use softmax for the last layer to get normalized 0-1 numbers\n",
    "        x = F.softmax(self.fc3(x), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network to operate on, a loss function, and an optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# use SGD to get to the right network weights\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Epoch 0 started-\n",
      "Minibatch 0\t loss: 1.7689617218971252\n",
      "Minibatch 1\t loss: 1.760445522069931\n",
      "Minibatch 2\t loss: 1.758530782699585\n",
      "Minibatch 3\t loss: 1.7608513691425323\n",
      "Minibatch 4\t loss: 1.7555492604970933\n",
      "Minibatch 5\t loss: 1.7630649082660674\n",
      "Minibatch 6\t loss: 1.7554638268351554\n",
      "-Epoch 1 started-\n",
      "Minibatch 0\t loss: 1.7493897597789765\n",
      "Minibatch 1\t loss: 1.7563895573616028\n",
      "Minibatch 2\t loss: 1.758571307182312\n",
      "Minibatch 3\t loss: 1.7499641982316971\n",
      "Minibatch 4\t loss: 1.7602580374479293\n",
      "Minibatch 5\t loss: 1.7508675047159195\n",
      "Minibatch 6\t loss: 1.7589246668815612\n",
      "-Epoch 2 started-\n",
      "Minibatch 0\t loss: 1.7519245581626892\n",
      "Minibatch 1\t loss: 1.749681302845478\n",
      "Minibatch 2\t loss: 1.7527376952767373\n",
      "Minibatch 3\t loss: 1.7476373903751374\n",
      "Minibatch 4\t loss: 1.7583470113277435\n",
      "Minibatch 5\t loss: 1.7586359758377075\n",
      "Minibatch 6\t loss: 1.7494117301702499\n",
      "-Epoch 3 started-\n",
      "Minibatch 0\t loss: 1.7532571882605552\n",
      "Minibatch 1\t loss: 1.7480087239146234\n",
      "Minibatch 2\t loss: 1.7539122959971427\n",
      "Minibatch 3\t loss: 1.752061693072319\n",
      "Minibatch 4\t loss: 1.7499438624978065\n",
      "Minibatch 5\t loss: 1.7534781628251075\n",
      "Minibatch 6\t loss: 1.7498207483291626\n",
      "-Epoch 4 started-\n",
      "Minibatch 0\t loss: 1.750680222928524\n",
      "Minibatch 1\t loss: 1.7464159559607506\n",
      "Minibatch 2\t loss: 1.7477323897480965\n",
      "Minibatch 3\t loss: 1.7498486530780792\n",
      "Minibatch 4\t loss: 1.7519584298729896\n",
      "Minibatch 5\t loss: 1.7522054686546327\n",
      "Minibatch 6\t loss: 1.7489710286855698\n",
      "-Epoch 5 started-\n",
      "Minibatch 0\t loss: 1.748122827708721\n",
      "Minibatch 1\t loss: 1.7527618305683137\n",
      "Minibatch 2\t loss: 1.7422778974175452\n",
      "Minibatch 3\t loss: 1.7491014501452447\n",
      "Minibatch 4\t loss: 1.7452795039415359\n",
      "Minibatch 5\t loss: 1.749003049314022\n",
      "Minibatch 6\t loss: 1.745752886235714\n",
      "-Epoch 6 started-\n",
      "Minibatch 0\t loss: 1.7438805506825448\n",
      "Minibatch 1\t loss: 1.7486498534083366\n",
      "Minibatch 2\t loss: 1.7490795459747315\n",
      "Minibatch 3\t loss: 1.7495767504572868\n",
      "Minibatch 4\t loss: 1.7471101034879684\n",
      "Minibatch 5\t loss: 1.7488003254532813\n",
      "Minibatch 6\t loss: 1.7542863681316376\n",
      "-Epoch 7 started-\n",
      "Minibatch 0\t loss: 1.750794152200222\n",
      "Minibatch 1\t loss: 1.744528271138668\n",
      "Minibatch 2\t loss: 1.7506305612921715\n",
      "Minibatch 3\t loss: 1.7519807887077332\n",
      "Minibatch 4\t loss: 1.752616607129574\n",
      "Minibatch 5\t loss: 1.7461960283517837\n",
      "Minibatch 6\t loss: 1.7479514281153679\n",
      "-Epoch 8 started-\n",
      "Minibatch 0\t loss: 1.745545952141285\n",
      "Minibatch 1\t loss: 1.749427534520626\n",
      "Minibatch 2\t loss: 1.7491182389259339\n",
      "Minibatch 3\t loss: 1.7458119280338287\n",
      "Minibatch 4\t loss: 1.7450266999602317\n",
      "Minibatch 5\t loss: 1.736990010201931\n",
      "Minibatch 6\t loss: 1.7507866635918616\n",
      "-Epoch 9 started-\n",
      "Minibatch 0\t loss: 1.7432474033236505\n",
      "Minibatch 1\t loss: 1.7435559589266778\n",
      "Minibatch 2\t loss: 1.7481958081126212\n",
      "Minibatch 3\t loss: 1.74870411837101\n",
      "Minibatch 4\t loss: 1.7421285510659217\n",
      "Minibatch 5\t loss: 1.7524041916131974\n",
      "Minibatch 6\t loss: 1.7428914266824722\n",
      "\n",
      "-Training completed-\n",
      "Data saved\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "num_epochs = 10\n",
    "\n",
    "# train `num_epochs` times\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    print(f\"-Epoch {epoch} started-\")\n",
    "    \n",
    "    # go over all minibatches each time\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get current minibatch data\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero out previous net weights: start from fresh state\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # do the forward pass and collect the loss from forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # do the backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # go on to next iteration in optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # print stats just for data logging\n",
    "        running_loss += loss.item()\n",
    "        # at every 2000 data pts in the current minibatch, print the loss\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'Minibatch {i // 2000}\\t loss: {running_loss/2000}')\n",
    "            running_loss = 0.0\n",
    "print(\"\\n-Training completed-\")\n",
    "\n",
    "            \n",
    "            \n",
    "# save the model at the end\n",
    "path = './mnist_net.pth'\n",
    "torch.save(net.state_dict(), path)\n",
    "print(\"Data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 87 %\n"
     ]
    }
   ],
   "source": [
    "# test network on test data\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        # get current minibatch data\n",
    "        images, labels = data\n",
    "        \n",
    "        # get the network predictions\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # add to logging\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
